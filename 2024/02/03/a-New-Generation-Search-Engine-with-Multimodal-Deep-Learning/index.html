<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><title>a New Generation Search Engine with Multimodal Deep Learning | Ankai Liang's Blog</title><noscript>Please enable JavaScript to view the site</noscript><link rel="icon" href="/img/pwa/favicon.png"><!-- index.css--><link rel="stylesheet" href="/css/index.css?v=1.7.8"><!-- inject head--><link rel="stylesheet" href="https://cdn2.codesign.qq.com/icons/7pOrz0WXB5ZWJPX/latest/iconfont.css"><!-- aplayer--><!-- swiper--><!-- Open Graph--><meta name="description" content="a New Generation Search Engine with Multimodal Deep LearningIntroductionWith a notable 91% of people desiring more video content from brands and a sig"><!-- pwa--><script>(win => {
        win.saveToLocal = {
            set: function setWithExpiry(key, value, ttl) {
                if (ttl === 0)
                    return
                const now = new Date()
                const expiryDay = ttl * 86400000
                const item = {
                    value: value,
                    expiry: now.getTime() + expiryDay,
                }
                localStorage.setItem(key, JSON.stringify(item))
            },

            get: function getWithExpiry(key) {
                const itemStr = localStorage.getItem(key)

                if (!itemStr) {
                    return undefined
                }
                const item = JSON.parse(itemStr)
                const now = new Date()

                if (now.getTime() > item.expiry) {
                    localStorage.removeItem(key)
                    return undefined
                }
                return item.value
            }
        }

        const DarkModeStatus = localStorage.getItem('theme')
        if (DarkModeStatus !== null) {
            if (DarkModeStatus === 'dark') {
                document.documentElement.setAttribute('data-theme', 'dark')
            } else {
                document.documentElement.setAttribute('data-theme', 'light')
            }
        }

        const asideStatus = saveToLocal.get('aside-status')
        if (asideStatus !== undefined) {
            if (asideStatus === 'hide') {
                document.documentElement.classList.add('hide-aside')
            } else {
                document.documentElement.classList.remove('hide-aside')
            }
        }

        win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
            const link = document.createElement('link')
            link.rel = 'stylesheet'
            link.href = url
            if (id) link.id = id
            link.onerror = reject
            link.onload = link.onreadystatechange = function () {
                const loadState = this.readyState
                if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
                link.onload = link.onreadystatechange = null
                resolve()
            }
            document.head.appendChild(link)
        })

        win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
            const script = document.createElement('script')
            script.src = url
            script.async = true
            script.onerror = reject
            script.onload = script.onreadystatechange = function () {
                const loadState = this.readyState
                if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
                script.onload = script.onreadystatechange = null
                resolve()
            }

            Object.keys(attr).forEach(key => {
                script.setAttribute(key, attr[key])
            })

            document.head.appendChild(script)
        })
    }
)(window)

console.log(
    "%c Program: Hexo %c Theme: Solitude %c Version: v1.7.8",
    "border-radius:5px 0 0 5px;padding: 5px 10px;color:white;background:#ff3842;",
    "padding: 5px 10px;color:white;background:#3e9f50;",
    "border-radius:0 5px 5px 0;padding: 5px 10px;background:#0084ff;color:white;"
);</script><!-- global head--><script>const GLOBAL_CONFIG = {
    root: '/',
    algolia: undefined,
    localsearch: undefined,
    runtime: '2017-02-20 00:00:00',
    lazyload: {
        enable: false,
        error: '/img/error_load.png'
    },
    copyright: false,
    highlight: {
        enable: true,
        limit: 200,
        expand: true,
        copy: true,
        syntax: 'highlight.js'
    },
    randomlink: false,
    lang: {"theme":{"dark":"dark","light":"light"},"copy":{"success":"Copied","error":"Copied failed"},"backtop":"Back to top","time":{"day":" days ago","hour":" hours ago","just":"just","min":" minutes ago","month":" months ago"},"f12":"Developer mode is turned on, please follow the GPL."},
    aside: {
        sayhello: {
            morning: '一日之计在于晨',
            noon: '吃饱了才有力气干活',
            afternoon: '集中精力，攻克难关',
            night: '不要太劳累了，早睡更健康',
            goodnight: '睡个好觉，保证精力充沛',
        },
        sayhello2: [],
    },
    covercolor: {
        enable: false
    },
    comment: {"avatar":"https://cravatar.cn","url":"https://waline-comment-five-smoky.vercel.app/","commentBarrage":true,"owo":{"body":".wl-emoji-popup","item":".wl-tab-wrapper button"}},
    lightbox: 'null',
    post_ai: {"key":"4ec1f0c37662c32a09a0","talk":"我是 Efu 开发的摘要生成助理EfuGPT，EfuGPT在静态部署时进行摘要的撰写，并且在访客访问时通过EfuCorrection转译后的文本摘要实现工具。我在这里只负责已经生成的摘要显示，你无法与我直接沟通，但我可以回答一些预设的问题。","randomPost":true}
};</script><meta name="generator" content="Hexo 7.1.1"></head><body id="body"><!-- universe--><canvas id="universe"></canvas><!-- loading--><!-- console--><div id="console"><div class="close-btn" onclick="sco.hideConsole()"><i class="solitude st-close-fill"></i></div><div class="button-group"><div class="console-btn-item"><span class="darkmode_switchbutton" onclick="sco.switchDarkMode()" title="Day and night switching"><i class="solitude st-moon-clear-fill"></i></span></div><div class="console-btn-item" id="consoleHideAside"><span class="asideSwitch" onclick="sco.switchHideAside()" title="Sidebar display control"><i class="solitude st-side-bar-fill"></i></span></div></div><div class="console-mask" onclick="sco.hideConsole()"></div></div><!-- sidebar--><div id="sidebar" style="zoom: 1;"><div id="menu-mask" style="display: none;"></div><div id="sidebar-menus"><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Archives</div><div class="length-num">11</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a></div></div></div><span class="sidebar-menu-item-title">Function</span><div class="sidebar-menu-item"><span class="darkmode_switchbutton menu-child" onclick="sco.switchDarkMode()"><i class="solitude st-moon-clear-fill"></i><span>Display mode</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span>Home</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>Articals</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="solitude  st-folder-fill"></i><span>Artical List</span></a></li><li><a class="site-page child" href="/tags/"><i class="solitude  st-price-tag-fill"></i><span>All tags</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><span>About</span></a></div></div><span class="sidebar-menu-item-title">Tags</span><div class="card-widget card-tags card-archives card-webinfo card-allinfo"><div class="card-tag-cloud"><a href="/tags/Distrubute-System/">Distrubute System<sup>2</sup></a><a href="/tags/System-Administration/">System Administration<sup>6</sup></a><a href="/tags/AI/">AI<sup>2</sup></a><a href="/tags/Search-Infra/">Search Infra<sup>1</sup></a><a href="/tags/AI-Search-Infra/">AI, Search Infra<sup>0</sup></a></div></div></div></div><!-- keyboard--><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav class="show" id="nav"><div id="nav-group"><div id="blog_name"><a id="site-name" href="/" title="Back to home"><span class="title">Ankai Liang / ex-Googler / ByteDancer</span></a></div><div id="page-name-mask"><div id="page-name"><a id="page-name-text" onclick="sco.toTop()">a New Generation Search Engine with Multimodal Deep Learning</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span>Home</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>Articals</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="solitude  st-folder-fill"></i><span>Artical List</span></a></li><li><a class="site-page child" href="/tags/"><i class="solitude  st-price-tag-fill"></i><span>All tags</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><span>About</span></a></div></div></div><div id="nav-left"></div><div id="nav-right"><div class="nav-button" id="nav-totop" onclick="sco.toTop()"><a class="totopbtn"><i class="solitude st-arrow-up-line"></i><span id="percent">0</span></a></div><div id="toggle-menu"><a class="site-page"><i class="solitude st-menu-line"></i></a></div></div></div></nav><div class="coverdiv" id="coverdiv"><img class="nolazyload" id="post-cover" src="/img/default.png" alt="a New Generation Search Engine with Multimodal Deep Learning"></div><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original" title="This article is a Original article, pay attention to the copyright.">Original</a><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/"><span class="tags-name tags-punctuation">AI</span></a></div></div></div></div><h1 class="post-title">a New Generation Search Engine with Multimodal Deep Learning</h1><div id="post-meta"><div class="meta-secondline"></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section></header><main class="layout" id="content-inner"><div id="post"><div class="post-ai"><div class="ai-title"><div class="ai-title-left"><i class="ai-title-icon solitude st-robot-fill"></i><div class="ai-title-text">文章摘要</div></div><div class="ai-tag" id="ai-tag">GPT 4</div></div><div class="ai-explanation" style="display: block;"></div><div class="ai-suggestions"></div><div class="ai-bottom"><div class="ai-tips"></div></div></div><article class="post-content" id="article-container"><h1 id="a-New-Generation-Search-Engine-with-Multimodal-Deep-Learning"><a href="#a-New-Generation-Search-Engine-with-Multimodal-Deep-Learning" class="headerlink" title="a New Generation Search Engine with Multimodal Deep Learning"></a>a New Generation Search Engine with Multimodal Deep Learning</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>With a notable 91% of people desiring more video content from brands and a significant 85% of American internet users engaging with online video, the digital sphere is clearly dominated by video content. Its influence is also seen in driving purchasing decisions. This rise underscores the value of video and emphasizes the growing necessity for effective video search tools.</p>
<p>Searching video content presents substantial challenges. Typically, traditional video searches rely on keyword queries that reference user-defined metadata — titles, tags, descriptions, etc. Unfortunately, the metadata of most video is sparse or non-existent due to the reliance on video creators to provide.</p>
<p>When encountering a dearth of metadata, machine learning models can enrich the available video context for enhanced searchability. These models, commonly used as annotators, generate a set of signals and tags. However, most of current solutions typically focus on a single modality, excelling at resolving specific tasks tied to particular types of input. </p>
<p>For example, ‘Video Captioning’ models adeptly describe the featured actions and behaviors in video content. Likewise, ‘Automatic Speech Recognition’ (ASR) models seamlessly convert speech into text while ‘Optical Character Recognition’ (OCR) models are proficient at recognizing text within images or videos. Some NLP models shine in summarizing and refining text information, yielding useful tags extracted from viewer comments.</p>
<p>Despite these advances, specific video search requirements can challenge even the most advanced models, especially when a search requires processing multiple modalities simultaneously. Imagine a user wanting to retrieve a specific video moment featuring a particular speaker discussing a specific topic in a designated location. Single modality models struggle with such requests due to their ability to process only one type of data at a time.</p>
<p>This is where my proposition comes into play: using multimodal models to fulfill both content summaries and enrich video context during the video search indexing process. By integrating and correlating information from various modalities, these models can deliver a more comprehensive and accurate understanding of video content. Thus, they can significantly enhance video search performance to meet complex user search demands.</p>
<h2 id="What-multimodal-models-can-bring-in-for-video-search"><a href="#What-multimodal-models-can-bring-in-for-video-search" class="headerlink" title="What multimodal models can bring in for video search"></a>What multimodal models can bring in for video search</h2><p>Multimodal models can be applied to multiple positions on the search link, such as query understanding and ranking, etc. This article only discusses its application in the indexing part, mainly reflected in the following tasks:</p>
<ul>
<li>Vision-Language<ul>
<li>Video-Text Retrieval: Mutual retrieval of image&#x2F;video and text.</li>
<li>Video Captioning: Generate text to describe the main content of a given image&#x2F;video.</li>
</ul>
</li>
<li>Localization tasks<ul>
<li>Temporal Language Localization: Given a video and a piece of text, locate the action described by the text (predict start and end time).</li>
<li>Video Summarization from Text Query: Given a query (sentence) and a video, summarize the video based on the content of the sentence, predicting key frames (or key segments) of the video to compose a short summary video.</li>
</ul>
</li>
</ul>
<p>The multimodal model can extract and understand diverse types of information from videos such as images, sounds, text data, and behavioral cues. To illustrate its advantage, consider a video capturing a Husky dog eating dog food and barking:</p>
<ol>
<li><p><strong>Enhanced precision</strong>: Traditional single-modal visual models may confuse a husky with a wolf due to visual similarities. However, the multimodal model, by integrating barking sounds, visual cues, and identifying the text on the dog food package, can surely recognize the breed.</p>
</li>
<li><p><strong>Deeper context recognition</strong>: The capability to process text information allows the multimodal model to identify the specific brand of dog food eaten by the Husky. This substantially enriches the indexed information, translating it to a richer search result.</p>
</li>
<li><p><strong>Noise reduction</strong>: Multimodal models excel at handling ‘noisy’ or ambiguous data. For instance, if the dog’s mouth is obscured in the video, traditional action recognition models might struggle to decipher the action. But, a multimodal model can use the “chewing sound” data to accurately infer that the dog is eating.</p>
</li>
</ol>
<h2 id="Technical-Challenges-for-Multimodal-Models"><a href="#Technical-Challenges-for-Multimodal-Models" class="headerlink" title="Technical Challenges for Multimodal Models"></a>Technical Challenges for Multimodal Models</h2><p><img src="/img/Challenges.png" alt="Technical Challenges for Multimodal Models"></p>
<h3 id="Representation"><a href="#Representation" class="headerlink" title="Representation"></a>Representation</h3><p>The first fundamental challenge resides in learning how to represent and summarize multimodal data, taking advantage of complementarity and redundancy among multiple modalities. The heterogeneity of multimodal data adds to the difficulty of constructing such representations. For instance, language is often symbolic, whereas audio and visual mediums are manifested as signals.</p>
<p>Single modality representations transform information into numerical vectors that computers can comprehend or further abstract into high-level feature vectors. Multimodal representations, in contrast, are about learning improved feature presentations by exploiting the complementarity among multiple modalities and eliminating the redundancy.</p>
<h3 id="Translation"><a href="#Translation" class="headerlink" title="Translation"></a>Translation</h3><p>The second challenge involves mapping data from one modality to another. Data is not only heterogeneous, but the relationships among modalities are also often complex or ambiguous. There exists many correct manners to describe an image, and maybe there’s no flawless translation. Generally, instance-based or model-driven methods are utilized, the specifics of which won’t be delved into here.</p>
<h4 id="Evaluation-Dilemma-in-Translation"><a href="#Evaluation-Dilemma-in-Translation" class="headerlink" title="Evaluation Dilemma in Translation"></a>Evaluation Dilemma in Translation</h4><p>The prime challenge for multimodal translation methods is that they are tough to evaluate. Tasks like speech recognition have only one correct translation; speech synthesis and media description tasks do not. As is the case in language translation, multiple responses can be correct, and determining which translation is superior is often subjective.</p>
<ul>
<li>Manual evaluation is the ideal test, but it is costly and requires a diverse group of raters to avoid bias.</li>
<li>Automatic metrics, including BLEU, Meteor, CIDEr, ROUGE, etc., are common alternatives in the image description realm, but their alignment with human assessment is proven weak.</li>
<li>Retrieval-based evaluations and simplifying tasks (e.g., transform the one-to-many mapping in image caption to one-to-one mapping in VQA) also act as solutions to the evaluation problem.</li>
</ul>
<h3 id="Alignment"><a href="#Alignment" class="headerlink" title="Alignment"></a>Alignment</h3><p>The third challenge resides in identifying direct relationships between sub-elements from two or more different modalities. For instance, one might wish to align the steps in a recipe with the corresponding segments of a video showing the dishes being prepared. To confront this challenge, we need to measure the similarity across different modalities and handle potential long-term dependencies and ambiguities.</p>
<h3 id="Fusion"><a href="#Fusion" class="headerlink" title="Fusion"></a>Fusion</h3><p>The fourth challenge is about making predictions by integrating information from two or more modalities. For instance, for audio-visual speech recognition, one might combine a visual description of the lip movements with the audio signal to predict the speech. Information from different modalities might have different predictive powers and noise topology, and data could be missing in at least one mode.</p>
<ul>
<li>Model-based Fusion Methods<ul>
<li>Deep Neural Networks: Enable end-to-end training incorporating LSTM, convolution layers, attention layers, gating mechanisms, and bilinear fusion to design intricate interactions of sequential data or image data.</li>
<li>Multiple Kernel Learning: Utilize different kernels for different data modalities&#x2F;views.</li>
<li>Graphical Models: Implement Hidden Markov models or Bayesian networks to model the joint probability distribution (generative) or conditional probability (discriminative).</li>
</ul>
</li>
</ul>
<h3 id="Co-learning"><a href="#Co-learning" class="headerlink" title="Co-learning"></a>Co-learning</h3><p>The fifth challenge is about transferring knowledge between the representations of modalities and their predictive models. Co-learning investigates how the knowledge learned from one modality can aid models trained over different modalities. This becomes critical when resources for one modality are scarce, like annotated data. The helper modality usually only participates in the model’s training phase, not in the testing phase.</p>
<h2 id="SOTA-Model-CLIP"><a href="#SOTA-Model-CLIP" class="headerlink" title="SOTA Model - CLIP"></a>SOTA Model - CLIP</h2><p><img src="/img/CLIP.png" alt="CLIP"></p>
<p>Let me introduce a cutting-edge SOTA model - CLIP (Contrastive Language-Image Pre-training). CLIP is OpenAI’s latest work combining NLP and CV, marking an important step forward in the multimodal field. Without using ImageNet’s data and labels for training, CLIP can still achieve the supervised training results of ResNet50 on the ImageNet dataset.</p>
<p>The primary contribution of CLIP is the adoption of unsupervised text information as supervision signals to learn visual features. </p>
<p>CLIP doesn’t predefine image and text label categories but directly uses the 400 million image-text pairs crawled from the Internet for training on image-text matching tasks. It has successfully applied the trained model to 30 existing computer vision categories.</p>
<h4 id="Phase-1-Contrastive-Pre-training"><a href="#Phase-1-Contrastive-Pre-training" class="headerlink" title="Phase 1: Contrastive Pre-training"></a>Phase 1: Contrastive Pre-training</h4><p>During the pre-training phase, contrastive learning is flexible - only positive sample-pairs and negative sample-pairs need to be defined. Image-text pairs that can be paired will serve as positive samples. </p>
<p><img src="/img/Contrastive%20pre-training.png" alt="Contrastive pre-training"></p>
<h4 id="Phase-2-Create-Dataset-Classifier-from-Label-Text"><a href="#Phase-2-Create-Dataset-Classifier-from-Label-Text" class="headerlink" title="Phase 2: Create Dataset Classifier from Label Text"></a>Phase 2: Create Dataset Classifier from Label Text</h4><p>Based on the learnt prior on the 400M data, impressive image classification abilities can be obtained just from the label text of the dataset. Now that the training is complete, we transition to the forward prediction stage, using the prompt label text to create a text feature vector for classification. </p>
<h4 id="Phase-3-Use-for-Zero-shot-Prediction"><a href="#Phase-3-Use-for-Zero-shot-Prediction" class="headerlink" title="Phase 3: Use for Zero-shot Prediction"></a>Phase 3: Use for Zero-shot Prediction</h4><p>Finally, it’s time to witness the effect of inference. For the test image, the category with the highest similarity is selected for output. During the inference stage, no matter what kind of image comes, as long as it is thrown into the Image Encoder for feature extraction, a one-dimensional image feature vector will be generated. Then this image feature is compared with N text features for cosine similarity, and the most similar one is the desired result, such as here, it should get “A photo of a guacamole.”</p>
<p><img src="/img/Use%20for%20zero-shot%20predictiion.png" alt="Use for zero-shot predictiion"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]<a target="_blank" rel="noopener" href="https://imzhanghao.com/2022/10/27/multimodal-learning/">multimodal-learning &#x2F; zhanghao &#x2F; blog</a></p>
<p>[2]<a target="_blank" rel="noopener" href="https://openai.com/blog/clip/">CLIP: Connecting Text and Images &#x2F; openai &#x2F; blog</a></p>
<p>[3]<a target="_blank" rel="noopener" href="https://github.com/yzhuoning/Awesome-CLIP">Awesome-CLIP &#x2F; yzhuoning</a></p>
</article><div class="post-copyright"><div class="post-copyright__author_group"><a class="post-copyright__author_img" href="/about/"><img class="post-copyright__author_img_front" src="/img/pwa/favicon.png"></a><div class="post-copyright__author_name">Ankai Liang's Blog</div><div class="post-copyright__author_desc">Strong but not aggressive, weak but not feeble.</div></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div id="quit-box" onclick="RemoveRewardMask()"></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">This piece of writing is an original article, utilizing the<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a>Agreement. For complete reproduction, please acknowledge the source as Courtesy of<a href="/">Ankai Liang's Blog</a></span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/"><span class="tags-punctuation"></span>AI<span class="tagsPageCount">2</span></a></div></div></div><nav class="needEndHide pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/08/17/Semi-Structured-Search/"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Semi-Structured Search</div></div></a></div></nav><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="solitude st-chat-fill"></i><span> Comment</span><span></span></div></div><div class="comment-wrap"><div id="comment"></div><script>async function initComment() {
    const isLazyLoad = !true;
    let walineInitFunction = window.walineFn || null;

    async function initWaline(Fn) {
        const walineOptions = {
            el: '#comment',
            serverURL: 'https://waline-comment-five-smoky.vercel.app/',
            pageview: !isLazyLoad,
            dark: 'html[data-theme="dark"]',
            path: window.location.pathname,
            comment: !isLazyLoad,
            ...null
        };
        const walineInstance = Fn(walineOptions);

        utils.addGlobalFn('pjax', () => walineInstance.destroy(), 'destroyWaline');
    }

    async function loadWalineCSSAndJS() {
        if (!walineInitFunction) {
            await getCSS('https://cdn.staticfile.net/waline/3.1.3/waline.min.css');
            const {init} = await import('https://cdn.staticfile.net/waline/3.1.3/waline.min.js');
            walineInitFunction = init || Waline.init;
            window.walineFn = walineInitFunction;
        }
        initWaline(walineInitFunction);
    }

    if (isLazyLoad) {
        utils.loadComment(document.getElementById('comment'),loadWalineCSSAndJS);
    } else {
        await loadWalineCSSAndJS();
    }

    if (GLOBAL_CONFIG.lightbox) {
        utils.lightbox(document.querySelectorAll('#comment .wl-content img:not(.wl-emoji)'));
    }

    sco.owoBig();
}</script></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><div class="author-info__top-group"><div class="author-info__sayhi" id="author-info__sayhi" onclick="sco.changeSayHelloText()">sayhello.morning</div></div></div><div class="avatar-img-group"><img class="avatar-img" alt="Avatar" src="/img/avatar.png"><div class="avatar-sticker"><img class="avatar-sticker-img" src="https://7.isyangs.cn/34/65f2e4e0423cc-34.png" alt="Mood sticker"></div></div><div class="author-info__description_group"><div class="author-info__description">Share my <b>journey</b> of exploring the ocean of knowledge.</div><div class="author-info__description2">Hope you find something you like</div></div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/about/"><div class="author-info__name">Ankai Liang</div><div class="author-info__desc">Strong but not aggressive, weak but not feeble.</div></a><div class="card-info-social-icons is-center"><a class="social-icon" target="_blank" rel="noopener" href="https://github.com/ankaiLiang" title="Github"><i class="solitude  st-github-line"></i></a><a class="social-icon" target="_blank" rel="noopener" href="https://www.instagram.com/ankai_liang/" title="Instagram"><i class="solitude  st-image-fill"></i></a><a class="social-icon" target="_blank" rel="noopener" href="https://www.linkedin.com/in/ankai-liang-a774b3119?trk=hp-identity-name" title="LinkedIn"><i class="solitude  st-contacts-fill"></i></a></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="solitude st-menu-line"></i><span>Table of contents</span></div><div class="toc-content" id="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#a-New-Generation-Search-Engine-with-Multimodal-Deep-Learning"><span class="toc-text">a New Generation Search Engine with Multimodal Deep Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-multimodal-models-can-bring-in-for-video-search"><span class="toc-text">What multimodal models can bring in for video search</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Technical-Challenges-for-Multimodal-Models"><span class="toc-text">Technical Challenges for Multimodal Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Representation"><span class="toc-text">Representation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Translation"><span class="toc-text">Translation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Evaluation-Dilemma-in-Translation"><span class="toc-text">Evaluation Dilemma in Translation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Alignment"><span class="toc-text">Alignment</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fusion"><span class="toc-text">Fusion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Co-learning"><span class="toc-text">Co-learning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SOTA-Model-CLIP"><span class="toc-text">SOTA Model - CLIP</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Phase-1-Contrastive-Pre-training"><span class="toc-text">Phase 1: Contrastive Pre-training</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Phase-2-Create-Dataset-Classifier-from-Label-Text"><span class="toc-text">Phase 2: Create Dataset Classifier from Label Text</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Phase-3-Use-for-Zero-shot-Prediction"><span class="toc-text">Phase 3: Use for Zero-shot Prediction</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="solitude st-map-line"></i><span>New posts</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/02/03/a-New-Generation-Search-Engine-with-Multimodal-Deep-Learning/" title="a New Generation Search Engine with Multimodal Deep Learning"><img alt="a New Generation Search Engine with Multimodal Deep Learning" src="/img/default.png"></a><div class="content"><a class="title" href="/2024/02/03/a-New-Generation-Search-Engine-with-Multimodal-Deep-Learning/" title="a New Generation Search Engine with Multimodal Deep Learning">a New Generation Search Engine with Multimodal Deep Learning</a></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/17/Semi-Structured-Search/" title="Semi-Structured Search"><img alt="Semi-Structured Search" src="/img/default4.png"></a><div class="content"><a class="title" href="/2023/08/17/Semi-Structured-Search/" title="Semi-Structured Search">Semi-Structured Search</a></div></div><div class="aside-list-item"><a class="thumbnail" href="/2017/08/17/DHT-REMOTE/" title="DHT-REMOTE"><img alt="DHT-REMOTE" src="/img/default3.png"></a><div class="content"><a class="title" href="/2017/08/17/DHT-REMOTE/" title="DHT-REMOTE">DHT-REMOTE</a></div></div><div class="aside-list-item"><a class="thumbnail" href="/2017/08/17/Page-Rank-Hadoop/" title="Page Rank - Hadoop"><img alt="Page Rank - Hadoop" src="/img/default3.png"></a><div class="content"><a class="title" href="/2017/08/17/Page-Rank-Hadoop/" title="Page Rank - Hadoop">Page Rank - Hadoop</a></div></div><div class="aside-list-item"><a class="thumbnail" href="/2017/08/17/RushHour/" title="RushHour"><img alt="RushHour" src="/img/default5.png"></a><div class="content"><a class="title" href="/2017/08/17/RushHour/" title="RushHour">RushHour</a></div></div></div></div></div></div></main><footer id="footer"><div id="st-footer-bar"><div class="footer-logo"><span class="solitude">Ankai Liang / ex-Googler / ByteDancer</span></div><div class="footer-bar-description">Article from Ankai Liang's Blog - Strong but not aggressive, weak but not feeble.</div><a class="footer-bar-link" href="/">Learn more</a></div><div id="footer_deal"><div class="nolazyload footer_mini_logo" id="footer_mini_logo" title="Back to top" onclick="sco.toTop()"><img src="/img/pwa/favicon.png" alt="Back to top"></div></div><div id="st-footer"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div class="copyright">© 2017 - 2024 By&nbsp;<a class="footer-bar-link" href="/">Ankai Liang</a></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/valor-x/hexo-theme-solitude" title="主题">主题</a><a class="footer-bar-link cc" href="/null" aria-label="copyright"><i class="solitude st-copyright-line"></i><i class="solitude st-creative-commons-by-line"></i><i class="solitude st-creative-commons-nc-line"></i><i class="solitude st-creative-commons-nd-line"></i></a></div></div></div></footer></div><!-- inject body--><div><script src="/js/main.js?v=1.7.8"></script><script src="/js/utils.js?v=1.7.8"></script><script src="/js/third_party/universe.min.js?v=1.7.8"></script><script>dark()
</script><script src="https://cdn.staticfile.net/pjax/0.2.8/pjax.min.js"></script><script src="https://cdn.staticfile.net/node-snackbar/0.1.16/snackbar.min.js"></script><script src="/js/third_party/efu_ai.min.js?v=1.7.8"></script><script src="https://cdn.staticfile.net/pace/1.2.4/pace.min.js"></script></div><div id="js-pjax"><script id="config-diff">var PAGE_CONFIG = {
    is_post: true,
    is_page: false,
    is_home: false,
    page: 'post',
    toc: true,
    comment: true,
}
</script></div><!-- newest comment--><!-- pjax--><script>let pjaxSelectors = [
    'title',
    '#body-wrap',
    '#site-config',
    'meta[name="description"]',
    '#js-pjax',
    'meta[property^="og:"]',
]

const pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
})

document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
})

document.addEventListener('pjax:complete', () => {
    window.refreshFn()
})

document.addEventListener('pjax:error', (e) => {
    if (e.request.status === 404) {
        pjax.loadUrl('/404.html')
    }
})</script><!-- theme--><script>initTheme = () => {
    let isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const cachedMode = saveToLocal.get('theme');
    if (cachedMode === undefined) {
        const nowMode =
            isDarkMode ? 'dark' : 'light'
        document.documentElement.setAttribute('data-theme', nowMode);
    } else {
        document.documentElement.setAttribute('data-theme', cachedMode);
    }
}
initTheme()</script><!-- google adsense--><!-- search--><!-- music--></body></html><script>const posts=["2024/02/03/a-New-Generation-Search-Engine-with-Multimodal-Deep-Learning/","2023/08/17/Semi-Structured-Search/","2017/08/17/DHT-REMOTE/","2017/08/17/Page-Rank-Hadoop/","2017/08/17/RushHour/","2017/08/17/Text-processing-in-Linux/","2017/08/17/afewmore-A-tool-of-ec2-how-to-write-a-shell-tool/","2017/08/16/tcpdump-1-DNS/","2017/08/16/Using-Manages-packages-in-Linux/","2017/08/16/Using-the-AWS-CLI-to-set-instance/","2017/05/19/Install and Configure AWS Tools/"];function toRandomPost(){ pjax.loadUrl('/'+posts[Math.floor(Math.random()*posts.length)]); }</script>